{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook investigates the generalization capability of our AI model in translating sign language. We train the model on datasets from different team members and then evaluate its performance on unseen data from other members. This approach allows us to understand the model's effectiveness in handling data variations and its reliability in real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Preprocessing Functions\n",
    "We define two key functions: `train_rnn_model` for training the RNN model on selected datasets and `preprocess_test_data` for preprocessing test data. These functions streamline the process of retraining the model on different datasets and preparing various test datasets for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import SimpleRNN, Bidirectional, BatchNormalization\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def train_rnn_model(*dataset_paths):\n",
    "\n",
    "     # Load and concatenate the datasets\n",
    "    dfs = [pd.read_csv(path) for path in dataset_paths]\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # number of rows and columns\n",
    "    print(df.shape)\n",
    "\n",
    "    # Convert all feature columns to numeric and set non-convertible values to NaN\n",
    "    for col in df.columns[:-1]:  # Excluding the last column\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Removing rows with NaN values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Separate features and labels\n",
    "    X = df.iloc[:, :-1].values  # All columns except the last one\n",
    "    y = df.iloc[:, -1].values   # Only the last column\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Reshape X to fit the RNN model (samples, time steps, features)\n",
    "    X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "    # Encode the labels\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "    # Define the RNN model\n",
    "    model_rnn = Sequential()\n",
    "    model_rnn.add(Bidirectional(SimpleRNN(30, activation='relu', return_sequences=True), input_shape=(X.shape[1], X.shape[2])))\n",
    "    model_rnn.add(BatchNormalization())\n",
    "    model_rnn.add(SimpleRNN(32, activation='relu'))\n",
    "    model_rnn.add(Dropout(0.3))\n",
    "    model_rnn.add(Dense(16, activation='relu'))\n",
    "    model_rnn.add(Dense(y_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "    # Compile the model with categorical_crossentropy loss function\n",
    "    model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Add EarlyStopping as a callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    model_rnn.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "    # Save the model\n",
    "    model_rnn.save('rnn_model.h5')\n",
    "    # Save the scaler to use it in predict.py and scale the realtime data\n",
    "    joblib.dump(scaler, 'rnn_scaler.joblib')\n",
    "\n",
    "    return model_rnn, scaler, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_data(df_test, scaler, encoder):\n",
    "    df_test.dropna(inplace=True)\n",
    "    X_test = df_test.iloc[:, :-1].values\n",
    "    y_test = df_test.iloc[:, -1].values\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
    "    return X_test, y_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_rnn, X, y_test):\n",
    "    y_pred = model_rnn.predict(X)\n",
    "    # Convert predictions to classes\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "    print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation on Unseen Data\n",
    "## First Test\n",
    "In this section, we evaluate the model's performance on data from Mouad, who was not included in the initial training set. This test aims to assess the model's ability to generalize from the training data to new, unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn, scaler, encoder = train_rnn_model('../dataset/sensor_data_badr.csv', '../dataset/sensor_data_ismail.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../dataset/sensor_data_mouad.csv')\n",
    "X_test_mouad, y_test_mouad_encoded = preprocess_test_data(df_test, scaler, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model_rnn, X_test_mouad, y_test_mouad_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Test\n",
    "Following the first test, we retrain the model on a larger dataset including Mouad's data and then test it on data from Kamal, another member not previously included. This step is crucial for assessing how well the model adapts to new individuals and the potential impact of dataset quality on model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn, scaler, encoder = train_rnn_model('../dataset/sensor_data_badr.csv', '../dataset/sensor_data_mouad.csv', '../dataset/sensor_data_ismail.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../dataset/sensor_data_kamal.csv')\n",
    "X_test_kamal, y_test_kamal_encoded = preprocess_test_data(df_test, scaler, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model_rnn, X_test_kamal, y_test_kamal_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The results from testing the model on unseen data highlight its capabilities and limitations in generalizing from the training dataset. While the model performed well on data from Mouad, the decreased accuracy observed with Kamal's data underscores the importance of dataset quality and diversity in training. These findings emphasize the need for comprehensive and varied training data to ensure the model's effectiveness across different individuals and sign language variations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
