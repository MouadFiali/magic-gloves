{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook extends our initial project on translating sign language. The focus here is on assessing the robustness of our model against potential sensor malfunctions. We simulate malfunctions by intentionally altering sensor data and evaluate the model's performance across various sensor combinations. This exercise aims to understand how sensor failures might impact the accuracy and reliability of our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Initial Model Development\n",
    "This section revisits the steps taken in our previous notebook, including data preprocessing, model building, and initial training. These foundational steps provide the context for our current focus on evaluating model robustness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import SimpleRNN, Bidirectional, BatchNormalization\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "df_1 = pd.read_csv('../dataset/sensor_data_badr.csv')\n",
    "df_2 = pd.read_csv('../dataset/sensor_data_mouad.csv')\n",
    "df_3 = pd.read_csv('../dataset/sensor_data_ismail.csv')\n",
    "# Concatenate the three dataframes\n",
    "df = pd.concat([df_1, df_2, df_3], ignore_index=True)\n",
    "\n",
    "# number of rows and columns\n",
    "print(df.shape)\n",
    "\n",
    "# Convert all feature columns to numeric and set non-convertible values to NaN\n",
    "for col in df.columns[:-1]:  # Excluding the last column\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Removing rows with NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.iloc[:, :-1].values  # All columns except the last one\n",
    "y = df.iloc[:, -1].values   # Only the last column\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape X to fit the RNN model (samples, time steps, features)\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Encode the labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Define the RNN model\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Bidirectional(SimpleRNN(30, activation='relu', return_sequences=True), input_shape=(X.shape[1], X.shape[2])))\n",
    "model_rnn.add(BatchNormalization())\n",
    "model_rnn.add(SimpleRNN(32, activation='relu'))\n",
    "model_rnn.add(Dropout(0.3))\n",
    "model_rnn.add(Dense(16, activation='relu'))\n",
    "model_rnn.add(Dense(y_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model with categorical_crossentropy loss function\n",
    "model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add EarlyStopping as a callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model_rnn.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Save the model\n",
    "model_rnn.save('rnn_model.h5')\n",
    "# Save the scaler to use it in predict.py and scale the realtime data\n",
    "joblib.dump(scaler, 'rnn_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensor Malfunction Simulation\n",
    "We simulate sensor malfunctions by intentionally setting the data for specific sensors to zero. This approach helps us to understand how the model performs when certain sensors fail or provide incorrect readings, a scenario that could occur in real-world usage.\n",
    "\n",
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def malfunction(X, sensor, value):\n",
    "    if sensor.startswith('Position') or sensor.startswith('Orientation'):\n",
    "        for col in df.columns:\n",
    "            # Get the column starting with the sensor name and has the hand name in it (left or right)\n",
    "            hand = sensor.split('-')[1]\n",
    "            if col.startswith(sensor.split('-')[0]) and hand in col:\n",
    "                X[:, :, df.columns.get_loc(col)] = value\n",
    "    else:\n",
    "        for col in df.columns:\n",
    "            if col.startswith(sensor):\n",
    "                X[:, :, df.columns.get_loc(col)] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    y_pred = model_rnn.predict(X)\n",
    "    # Convert predictions to classes\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "    print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation on Malfunctioned Data\n",
    "In this section, we loop over various combinations of malfunctioning sensors and evaluate the model's performance. This process involves adjusting the data to simulate different malfunction scenarios and then computing key metrics like accuracy, precision, recall, and F1-score for each scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "sensors = []\n",
    "for i in range(1, 6):\n",
    "    sensors.append(f'Flex-Left-{i}')\n",
    "    sensors.append(f'Flex-Right-{i}')\n",
    "sensors.append(f'Position-Left')\n",
    "sensors.append(f'Position-Right')\n",
    "sensors.append(f'Orientation-Left')\n",
    "sensors.append(f'Orientation-Right')\n",
    "\n",
    "combinations = []\n",
    "for i in range(1, len(sensors) + 1):\n",
    "    # Get all combinations of sensors\n",
    "    combinations += list(itertools.combinations(sensors, i))\n",
    "\n",
    "print(f\"Number of combinations: {len(combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to try combinations of sensor values by destroying other sensors (try all combinations)\n",
    "# Meaning that we will first try to remove 1 sensor, then 2 sensors (with all possible combinations), then 3 sensors, and so on\n",
    "# The function will return the accuracy, precision, recall, and F1-score for each combination sorted by accuracy\n",
    "import itertools\n",
    "\n",
    "\n",
    "def try_combinations(X, y_test):\n",
    "    # Get the names of the sensors\n",
    "    sensors = []\n",
    "    for i in range(1, 6):\n",
    "        sensors.append(f'Flex-Left-{i}')\n",
    "        sensors.append(f'Flex-Right-{i}')\n",
    "    sensors.append(f'Position-Left')\n",
    "    sensors.append(f'Position-Right')\n",
    "    sensors.append(f'Orientation-Left')\n",
    "    sensors.append(f'Orientation-Right')\n",
    "    \n",
    "    # Create a list to store the results\n",
    "    results = []\n",
    "    # Loop over the sensors\n",
    "    for i in range(1, len(sensors) + 1):\n",
    "        # Get all combinations of sensors\n",
    "        combinations = list(itertools.combinations(sensors, i))\n",
    "        # Loop over the combinations\n",
    "        for combination in combinations:\n",
    "            # Create a copy of the test set\n",
    "            X_test_copy = X.copy()\n",
    "            # Destroy the sensors in the combination\n",
    "            for sensor in combination:\n",
    "                malfunction(X_test_copy, sensor, 0)\n",
    "            # Predict the labels\n",
    "            y_pred = model_rnn.predict(X_test_copy)\n",
    "            # Convert predictions to classes\n",
    "            y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "            y_test_classes = np.argmax(y_test, axis=1)\n",
    "            # Calculate the accuracy\n",
    "            accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "            # Calculate precision, recall, and F1-score\n",
    "            precision = precision_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "            recall = recall_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "            f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "            # Append the results to the list\n",
    "            results.append([combination, accuracy, precision, recall, f1])\n",
    "            print(\"************************************\")\n",
    "            print(f\"Combination {combination}:\")\n",
    "            print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    # Sort the results by accuracy\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = try_combinations(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Storage and Comprehensive Analysis\n",
    "The results of our evaluations are stored for detailed analysis. This analysis aims to identify patterns in the model's performance degradation related to specific sensor failures, providing insights into the model's robustness and areas for improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a csv file\n",
    "df_results = pd.DataFrame(results, columns=['Combination', 'Accuracy', 'Precision', 'Recall', 'F1-score'])\n",
    "df_results.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the combination with accuracy > 0.95 and print its combination and accuracy\n",
    "max_combination = 0\n",
    "for result in results:\n",
    "    if result[1] > 0.95 and len(result[0]) > max_combination:\n",
    "        max_combination = len(result[0])\n",
    "        print(f\"Combination: {result[0]}\")\n",
    "        print(f\"Accuracy: {result[1] * 100:.2f}%\")\n",
    "\n",
    "print(f\"Number of sensors: {max_combination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Our investigation into the model's robustness against sensor malfunctions has provided valuable insights. The results indicate how sensor failures impact the overall performance, highlighting the need for fault-tolerant designs in practical applications. Furthermore, through this evaluation, we have gained a clearer understanding of the relative importance of each sensor in our device. We identified certain sensors that do not significantly contribute to the model's predictive ability. This knowledge is invaluable, as it allows us to optimize our design by focusing on the most critical sensors and considering the removal or replacement of sensors that add minimal value. This streamlining could lead to more efficient and cost-effective designs without compromising the functionality and accuracy of our sign language translation tool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
