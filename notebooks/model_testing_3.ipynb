{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook explores the impact of sensor reduction on the model's ability to differentiate between similar sign language movements. By limiting the data to a specific combination of sensors that remain consistent across movements, we aim to understand how sensor selection influences the model's accuracy and ability to distinguish between similar signs, based on out current dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lenovo\\Desktop\\ENSIMAG\\IA & IoT\\magic-gloves\\magic-gloves-env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import SimpleRNN, Bidirectional, BatchNormalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Columns Reduction\n",
    "In this section, we use a function to prepare our existing dataset, on which the model will be training, by selecting a specific combination of columns. This selection will, in the next sections, be adapted to two scenarios : Sensors' Reduction and Recording Frequency Deceleration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(dataframes, prefixes, suffixes):\n",
    "    # Concatenate the dataframes\n",
    "    df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Identify columns to keep based on sensor names\n",
    "    cols_to_keep = []\n",
    "    for prefix in prefixes or ['']:\n",
    "        for suffix in suffixes or ['']:\n",
    "            cols_to_keep.extend([col for col in df.columns if col.startswith(prefix) and col.endswith(suffix)])\n",
    "\n",
    "    # Add the last column to the list of columns to keep\n",
    "    cols_to_keep.append(df.columns[-1])\n",
    "\n",
    "    # Keep only the columns corresponding to the sensors in sensors_to_keep\n",
    "    df = df[cols_to_keep]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataframes, prefixes, suffixes):\n",
    "\n",
    "    # Keep only the columns corresponding to the sensors in sensors_to_keep\n",
    "    df = prepare_dataframe(dataframes, prefixes, suffixes)\n",
    "\n",
    "    # Convert all feature columns to numeric and set non-convertible values to NaN\n",
    "    for col in df.columns[:-1]:  # Excluding the last column\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Removing rows with NaN values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Separate features and labels\n",
    "    X = df.iloc[:, :-1].values  # All columns except the last one\n",
    "    y = df.iloc[:, -1].values   # Only the last column\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Reshape X to fit the RNN model (samples, time steps, features)\n",
    "    X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "    # Encode the labels\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "    # Define the RNN model\n",
    "    model_rnn = Sequential()\n",
    "    model_rnn.add(Bidirectional(SimpleRNN(30, activation='relu', return_sequences=True), input_shape=(X.shape[1], X.shape[2])))\n",
    "    model_rnn.add(BatchNormalization())\n",
    "    model_rnn.add(SimpleRNN(32, activation='relu'))\n",
    "    model_rnn.add(Dropout(0.3))\n",
    "    model_rnn.add(Dense(16, activation='relu'))\n",
    "    model_rnn.add(Dense(y_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "    # Compile the model with categorical_crossentropy loss function\n",
    "    model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Add EarlyStopping as a callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    history = model_rnn.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('../dataset/sensor_data_badr.csv')\n",
    "df_2 = pd.read_csv('../dataset/sensor_data_mouad.csv')\n",
    "df_3 = pd.read_csv('../dataset/sensor_data_ismail.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation with Reduced Sensor Data\n",
    "Here, we retrain our model using the reduced sensor dataset. The columns' selection is based on the hypothesis that these sensors will provide similar readings across different sign language movements, thereby challenging the model's differentiation capability. The focus is to observe how the model performs when provided with data that is potentially less distinctive between different signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\Desktop\\ENSIMAG\\IA & IoT\\magic-gloves\\magic-gloves-env\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/45 [==============================] - 3s 13ms/step - loss: 1.5234 - accuracy: 0.3177 - val_loss: 1.7710 - val_accuracy: 0.1676\n",
      "Epoch 2/100\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.2869 - accuracy: 0.4979 - val_loss: 1.5744 - val_accuracy: 0.3883\n",
      "Epoch 3/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 1.0541 - accuracy: 0.5880 - val_loss: 1.2850 - val_accuracy: 0.5279\n",
      "Epoch 4/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.9190 - accuracy: 0.6020 - val_loss: 1.0597 - val_accuracy: 0.5475\n",
      "Epoch 5/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.8599 - accuracy: 0.6138 - val_loss: 0.9594 - val_accuracy: 0.6089\n",
      "Epoch 6/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.8268 - accuracy: 0.6306 - val_loss: 0.8693 - val_accuracy: 0.6341\n",
      "Epoch 7/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.7578 - accuracy: 0.6522 - val_loss: 0.8409 - val_accuracy: 0.6313\n",
      "Epoch 8/100\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.7667 - accuracy: 0.6522 - val_loss: 0.7958 - val_accuracy: 0.6480\n",
      "Epoch 9/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.7432 - accuracy: 0.6641 - val_loss: 0.8835 - val_accuracy: 0.5810\n",
      "Epoch 10/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.7404 - accuracy: 0.6627 - val_loss: 0.7033 - val_accuracy: 0.6732\n",
      "Epoch 11/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.7312 - accuracy: 0.6795 - val_loss: 0.6495 - val_accuracy: 0.7067\n",
      "Epoch 12/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.7081 - accuracy: 0.6739 - val_loss: 0.6803 - val_accuracy: 0.6872\n",
      "Epoch 13/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.7044 - accuracy: 0.6976 - val_loss: 0.7162 - val_accuracy: 0.6760\n",
      "Epoch 14/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.7183 - accuracy: 0.6788 - val_loss: 0.6422 - val_accuracy: 0.7011\n",
      "Epoch 15/100\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.6645 - accuracy: 0.7102 - val_loss: 0.6001 - val_accuracy: 0.7486\n",
      "Epoch 16/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6850 - accuracy: 0.7025 - val_loss: 0.6268 - val_accuracy: 0.7095\n",
      "Epoch 17/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6610 - accuracy: 0.7074 - val_loss: 0.5885 - val_accuracy: 0.7514\n",
      "Epoch 18/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6747 - accuracy: 0.7053 - val_loss: 0.6323 - val_accuracy: 0.7123\n",
      "Epoch 19/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6813 - accuracy: 0.6885 - val_loss: 0.5903 - val_accuracy: 0.7430\n",
      "Epoch 20/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6399 - accuracy: 0.7060 - val_loss: 0.7204 - val_accuracy: 0.6927\n",
      "Epoch 21/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6766 - accuracy: 0.6955 - val_loss: 0.6136 - val_accuracy: 0.6983\n",
      "Epoch 22/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6605 - accuracy: 0.7053 - val_loss: 0.5775 - val_accuracy: 0.7626\n",
      "Epoch 23/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6757 - accuracy: 0.7053 - val_loss: 0.6419 - val_accuracy: 0.6899\n",
      "Epoch 24/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6515 - accuracy: 0.7207 - val_loss: 0.6704 - val_accuracy: 0.6872\n",
      "Epoch 25/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6362 - accuracy: 0.7165 - val_loss: 0.5610 - val_accuracy: 0.7682\n",
      "Epoch 26/100\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.6379 - accuracy: 0.7242 - val_loss: 0.7020 - val_accuracy: 0.6899\n",
      "Epoch 27/100\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.6657 - accuracy: 0.7032 - val_loss: 0.5812 - val_accuracy: 0.7346\n",
      "Epoch 28/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6405 - accuracy: 0.7200 - val_loss: 0.5679 - val_accuracy: 0.7374\n",
      "Epoch 29/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6258 - accuracy: 0.7186 - val_loss: 0.6407 - val_accuracy: 0.6760\n",
      "Epoch 30/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6279 - accuracy: 0.7304 - val_loss: 0.5691 - val_accuracy: 0.7486\n",
      "Epoch 31/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6336 - accuracy: 0.7228 - val_loss: 0.6004 - val_accuracy: 0.7374\n",
      "Epoch 32/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6226 - accuracy: 0.7284 - val_loss: 0.6826 - val_accuracy: 0.6927\n",
      "Epoch 33/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6466 - accuracy: 0.6955 - val_loss: 0.5941 - val_accuracy: 0.7179\n",
      "Epoch 34/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6156 - accuracy: 0.7291 - val_loss: 0.6480 - val_accuracy: 0.7095\n",
      "Epoch 35/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5931 - accuracy: 0.7353 - val_loss: 0.5831 - val_accuracy: 0.7542\n",
      "Train loss:  0.5930998921394348\n",
      "Validation loss:  0.5831094980239868\n",
      "Train accuracy:  0.7353351712226868\n",
      "Validation accuracy:  0.7541899681091309\n"
     ]
    }
   ],
   "source": [
    "prefixes = ['Flex-Right-1']\n",
    "\n",
    "history = train_model([df_1, df_2, df_3], prefixes, None)\n",
    "\n",
    "# Access the loss and accuracy values\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "print('Train loss: ',train_loss[-1])\n",
    "print('Validation loss: ', val_loss[-1])\n",
    "print('Train accuracy: ', train_accuracy[-1])\n",
    "print('Validation accuracy: ', val_accuracy[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training with Reduced Recording Frequency\n",
    "Here, we retrain our model using the reduced recording frequency. This means that the columns' selection is based on the recution of the number of frames for each sign, thereby challenging the model's robustness. The focus is to observe how the model performs when provided with less detailed sequence of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impement a function that takes an integer n as parameters, and appends to a list the strings 'Frame-k', where k%n == 0 and k<=20\n",
    "# In order to simulate reduced recording frequency\n",
    "def get_frame_names(n):\n",
    "    frame_names = []\n",
    "    for k in range(1, 21):\n",
    "        if k % n == 0:\n",
    "            frame_names.append('Frame-' + str(k))\n",
    "    return frame_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Frame-4', 'Frame-8', 'Frame-12', 'Frame-16', 'Frame-20']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\Desktop\\ENSIMAG\\IA & IoT\\magic-gloves\\magic-gloves-env\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/45 [==============================] - 3s 12ms/step - loss: 1.4083 - accuracy: 0.4672 - val_loss: 1.5486 - val_accuracy: 0.4190\n",
      "Epoch 2/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.8492 - val_loss: 0.9607 - val_accuracy: 0.8771\n",
      "Epoch 3/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2734 - accuracy: 0.9469 - val_loss: 0.4379 - val_accuracy: 0.9721\n",
      "Epoch 4/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1261 - accuracy: 0.9777 - val_loss: 0.1575 - val_accuracy: 0.9888\n",
      "Epoch 5/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0694 - accuracy: 0.9909 - val_loss: 0.0492 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9937 - val_loss: 0.0215 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0376 - accuracy: 0.9951 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.9979 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.9944 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.9951 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.9958 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.9951 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.9979 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 4.5828e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.9951 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.9993 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0094 - accuracy: 0.9986 - val_loss: 4.5493e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0068 - accuracy: 0.9993 - val_loss: 4.9851e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0078 - accuracy: 0.9986 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.9979 - val_loss: 4.9759e-04 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 0.9972\n",
      "Epoch 22/100\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.9993 - val_loss: 0.0033 - val_accuracy: 0.9972\n",
      "Epoch 23/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0106 - accuracy: 0.9972 - val_loss: 0.0163 - val_accuracy: 0.9972\n",
      "Epoch 24/100\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 5.9922e-04 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.5755e-04 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0069 - accuracy: 0.9986 - val_loss: 9.1635e-04 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 0.9986 - val_loss: 5.9700e-04 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.2482e-04 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 3.4533e-05 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 4.7690e-05 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 6.6233e-05 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 3.4046e-04 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 5.9773e-05 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0063 - accuracy: 0.9986 - val_loss: 8.2376e-05 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 9.3974e-05 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 8.2514e-05 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0055 - accuracy: 0.9993 - val_loss: 6.3287e-05 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Train loss:  0.006376589648425579\n",
      "Validation loss:  0.0012447142507880926\n",
      "Train accuracy:  0.9979050159454346\n",
      "Validation accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "suffixes = get_frame_names(4)\n",
    "print(suffixes)\n",
    "\n",
    "history = train_model([df_1, df_2, df_3], None, suffixes)\n",
    "\n",
    "# Access the loss and accuracy values\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "print('Train loss: ',train_loss[-1])\n",
    "print('Validation loss: ', val_loss[-1])\n",
    "print('Train accuracy: ', train_accuracy[-1])\n",
    "print('Validation accuracy: ', val_accuracy[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The results of this experiment underscore the importance of sensor diversity in the precise interpretation of sign language. The model's performance under reduced sensor data underscores the difficulty in distinguishing similar signs and suggests a potential requirement for a comprehensive sensor setup to attain optimal accuracy. Additionally, the model maintained its high performance even when presented with a less detailed sequence of frames, supporting the hypothesis that the model's accuracy is attributable to the distinct nature of the performed signs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magic-gloves-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
