<!DOCTYPE html>
<html>

<head>
    <title>Magic Gloves</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <style>
        #pills-tab {
            display: flex;
            justify-content: center;
        }
        #probleme,
        #donnees,
        #EvaluationEnv,
        #bib,
        #Methodes {
            margin-left: 150px;
            margin-right: 150px;
        }

        h4 {
            margin-left: 40px;
        }

        h3 {
            margin-left: 20px;
        }

        h2 {
            display: flex;
            justify-content: center;
        }

        .menu {
            position: fixed;
            top: 0px;
            left: 0;
            z-index: 1000;
            text-align: center;
            background-color: #333;
            width: 100%;
        }

        .menu ul {
            list-style-type: none;
            padding: 0;
            margin: 0;
            display: flex;
            justify-content: center;
        }

        .menu ul li {
            display: inline-block;
        }

        .menu ul li a {
            display: block;
            color: white;
            text-align: center;
            padding: 14px 16px;
            text-decoration: none;
        }

        .menu ul li a:hover:not(.active) {
            background-color: #111;
        }

        #content {
            margin-left: 100px;
            margin-right: 100px;
        }

        #notebooks-link {
            display: flex;
            justify-content: center;
            align-items: center;
            font-size: 20px;
            font-weight: bold;
            margin-left: 50px;
        }

        table {
            margin-left: auto;
            margin-right: auto;
            font-size: 20px;
            font-weight: bold;
        }

        .text-td {
            text-align: start;
            vertical-align: middle;
        }

        .img-td {
            text-align: center;
            vertical-align: middle;
        }

        td img {
            height: 70px;
        }

        .welcome {
            display: flex;
            justify-content: center;
            align-items: center;
        }
        #creators {
            display: flex;
            justify-content: center;
        }
        .welcome img {
            width: 150px;
            height: 150px;
            object-fit: cover;
        }

        .intro {
            margin-left: 50px;
            margin-right: 50px;
        }
        body {
            margin-top: 80px;
            counter-reset: bibliography; 
        }
        .bibliography {
            list-style-type: none;
            padding: 0;
        }
        .bibliography li {
            counter-increment: bibliography; 
            margin-bottom: 10px;
        }
        .bibliography li:before {
            content: "[" counter(bibliography) "] "; 
            font-weight: bold;
        }
        .bibliography .citation {
            font-family: 'Times New Roman', Times, serif;
            font-size: 14px;
        }
        .bibliography .authors {
            font-weight: bold;
        }
        .bibliography .title {
            font-style: italic;
        }
        .reference {
            font-weight: bold;
        }
    </style>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.7/dist/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>
    <script>
        function showContent(contentId) {
            var contentRapport = ` <section id=\"probleme\"><h1>1. Présentation du problème traité</h1> <h3>1.1. Contexte du problème :</h3> 
                &nbsp; &nbsp; Le langage des signes est essentiel pour les personnes confrontées à des déficiences auditives, telles que la surdité, ainsi que pour celles atteintes de mutisme. Cependant, cette forme de communication n'est pas largement maîtrisée par la majorité de la population, ce qui crée une barrière significative dans les interactions. Cette situation ne limite pas seulement les échanges sociaux de base, mais restreint aussi l'accès à des services clés pour ces communautés.
                L'insuffisance de moyens de communication adaptés est exacerbée par le manque d'outils et de technologies conçus pour faciliter l'interaction entre les personnes utilisant le langage des signes et celles qui ne le comprennent pas. Cette carence renforce l'isolement et limite la participation active de ces individus dans la société.
                <h3>1.2. Impact personnel et sociétal :</h3> 
                &nbsp; &nbsp; L'absence d'outils adaptés pour la communication entre les personnes sourdes, malentendantes ou muettes et le reste de la population conduit à des malentendus et crée des barrières dans divers contextes, tels que les interactions médicales, professionnelles et éducatives.
                
                Cette situation a un impact profond sur la qualité de vie de ces personnes, engendrant un sentiment d'isolement et limitant leur participation active dans la société. Elle souligne l'urgence de développer des solutions innovantes et inclusives qui permettent de surmonter ces obstacles communicatifs, ouvrant ainsi la voie à une société plus intégrée et accessible à tous, indépendamment des défis auditifs ou de parole.

                <h3>1.3. Objectif du projet \'Magic Gloves\' :</h3>
                &nbsp; &nbsp; Le projet \'Magic Gloves\' vise à développer une solution technologique pour surmonter cette barrière. En utilisant des gants innovants équipés de capteurs flexion, des acceléromètres et gyroscopes, le projet propose de traduire les mouvements du langage des signes en parole écrite intelligible, rendant la communication plus accessible et inclusive. Cette innovation a le potentiel de transformer radicalement la manière dont les personnes sourdes ou malentendantes interagissent avec le monde qui les entoure. 
                </br></br>
                <div style="display: flex; justify-content: center; align-items: center;">
                    <img src="https://github.com/MouadFiali/magic-gloves/blob/main/images/Gants_2.jpeg?raw=true" width="500px" alt="Magic Gloves logo">
                </div>
                </br>
                </section>
                

            
                </section>
                <section id="donnees">
                    <h1>2. Les données utilisées</h1>
                    <h3>2.1. Limitations des données existantes</h3>
                    &nbsp;&nbsp;L'utilisation d'un jeu de données existant semblait initialement être une solution pratique pour notre projet. Nous avons notamment considéré un jeu de données sur Kaggle, intitulé <i>"<b>Leap Motion ASL and BSL Sign Language Recognition</b>"</i> <a href="#kaggle-dataset" class="reference">[1]</a>, qui semblait prometteur. Cependant, plusieurs obstacles ont rendu cette option peu viable pour nos besoins spécifiques.
                    </br>
                    &nbsp;&nbsp;La différence fondamentale entre les données du jeu de données trouvé sur Kaggle et celles capturées par nos capteurs posait un problème significatif. Adapter ces données à nos outils nécessitait des recalibrages complexes et des ajustements algorithmiques pour aligner dimensions et échelles.
                    </br>
                    &nbsp;&nbsp;Tous ces défis, liés à l'adaptation de ces données à nos capteurs spécifiques, ainsi que les complexités mathématiques requises pour une telle transformation, nous ont rapidement orientés vers une deuxième solution qui est: La création d'un jeu de données convenable. En effet, la personnalisation était nécessaire pour assurer une compatibilité optimale avec nos outils et capteurs.
                    </br>
                    <h3>2.2. Conception d'un jeu de données pour la reconnaissance de mouvements des mains</h3>
                    &nbsp; &nbsp; Lors de la conception de notre ensemble de données, nous avons accordé une attention particulière à sa structuration et à son organisation, en veillant à optimiser la lisibilité et la compréhensibilité pour nos modèles d'intelligence artificielle. Cela impliquait une organisation réfléchie des noms de colonnes et une considération minutieuse pour leur agencement logique, ainsi que l'attention portée aux aspects sémantiques. Notre approche visait à rendre le jeu de données intuitif pour les algorithmes d'apprentissage automatique, en alignant les données de manière à répondre efficacement aux exigences des modèles prédictifs. Cette structuration méthodique se manifeste à travers plusieurs aspects clés, garantissant ainsi une intégration harmonieuse des données dans le processus d'analyse. Voici une explication détaillée de sa structure :
                    </br></br>
                        <div class="dataFlex">
                            <ul>
                                <li><b><span style="background-color: #F2F2F2;">Flex-Left-p-Frame-n : </span></b> Avec n variant de 1 à 20, représentant différents moments (frames) de l'enregistrement, et p variant de 1 à 5 représentant les doigts de la main gauche. Ces colonnes mesurent le degré de flexion de chaque doigt de la main gauche sur 20 frames.
                                </li></br>
                                <li><b><span style="background-color: #F2F2F2;">Acceleration-X/Y/Z-Left-Frame-n  : </span></b> Avec n variant de 1 à 20 frames de manière similaire. Ces colonnes indiquent l'accélération de chaque main pendant les 20 frames, selon les axes X, Y et Z, fournissant également une idée de l'accéleration de la main.
                                </li></br>
                                <li><b><span style="background-color: #F2F2F2;">Orientation-X/Y/Z-Left-Frame-n : </span></b> Ces colonnes, également avec n variant de 1 à 20 frames, indiquent l'orientation de la main gauche dans chaque frame, dans les trois directions X, Y et Z.
                                </li></br>
                                <li><b><span style="background-color: #F2F2F2;">Acceleration-X/Y/Z-Right-Frame-n | Orientation-X/Y/Z-Right-Frame-n | Flex-Right-p-Frame-n : </span></b> Ces colonnes sont analogues aux paramètres de la main gauche, mais pour la main droite.
                                </li>
                            </ul>
                        </div>
                    &nbsp; &nbsp; Notre conception des colonnes pour le jeu de données s'inspire du dataset "Leap Motion ASL and BSL Sign Language Recognition" mentionnée avant. Cependant, nous avons adapté et affiné cette structure pour mieux correspondre à nos besoins et à nos capteurs. Nous avons adopté une structure basée sur la division du mouvement des mains en plusieurs instantanés ou 'frames' (Frame-n). Chaque 'frame' saisit un moment précis du mouvement, permettant de documenter les dynamiques et nuances des signes. La segmentation en frames assure que nous suivons le mouvement des mains du début à la fin du signe, ce qui est indispensable pour interpréter correctement des signes complexes qui évoluent sur des durées plus étendues.
                    </br></br>
                    &nbsp; &nbsp; En prenant en considération les différentes catégories de données pour chaque main, notamment la flexion, la position et l'orientation pour les deux mains sur 20 frames, le nombre total de colonnes de données atteint 440 colonnes. Chaque colonne représente une caractéristique spécifique des mouvements des mains. En complément de ces 440 colonnes, une dernière colonne nommée "SIGN" a été intégrée. Cette dernière servira de colonne cible (target) pour notre modèle d'apprentissage automatique, indiquant le signe correspondant à chaque ensemble de mouvements enregistrés.
                    
                    <h3>2.3. Processus d'enregistrement des données</h3>
                    &nbsp; &nbsp; Dans la configuration actuelle de notre ensemble de données (voir Annexe), notre équipe a réussi à enregistrer près de 2600 instances, couvrant six signes différents avec environ 400 enregistrements pour chaque signe et un signe de "pause": Pour faciliter la compréhension et l'analyse des données, nous avons introduit un signe de pause qui s'ajoute aux six signes principaux lorsque l'utilisateur ne réalise aucun mouvement. Cette collecte a été effectuée grâce à un processus rigoureux, impliquant la participation de divers intervenants au sein de notre équipe pour garantir une représentation diversifiée des signes.
                    </br></br>
                    &nbsp; &nbsp; Ce processus a nécessité l'utilisation d'un script injecté à un Arduino UNO pour capturer en temps réel les données des capteurs, chaque enregistrement étant converti en une série de valeurs séparées par des virgules. Ces données sont ensuite traitées par un script Python qui les restructure pour les aligner sur les colonnes de notre ensemble de données, avant d'être sauvegardées au format CSV. Chaque ligne de ce fichier CSV représente une série de 20 frames de données de capteurs, avec une pause également enregistrée lorsqu'aucun signe n'est effectué.
                    </br></br>
                    &nbsp; &nbsp; Bien que notre jeu de données actuelle soit petite, elle nous permet de faire la preuve de concept nécessaire pour montrer le fonctionnement. Cependant, pour mettre ce projet en utilisation réelle, il faudra un jeu de données plus large et variée, ce qui implique un investissement de temps significatif, étant donné que nous sommes responsables de l'enregistrement des données.
                    </br></br>
                </section>

                <section id="Methodes">
                    <h1>3. Les Méthodes utilisées</h1>

                        <h3>3.1. RNN et LSTM : Choix des Modèles</h3>
                            <p>
                                &nbsp; &nbsp; Dans le cadre du projet "Magic Gloves", deux architectures principales d'intelligence artificielle ont été choisies : les Réseaux de Neurones Récurrents (RNN) et les Long Short-Term Memory (LSTM). 
                            </p>
                        <h4>3.1.1. RNN :</h4>
                        <ul>
                            <li><strong>Principe :</strong> Les RNN sont conçus pour traiter des séquences de données, en conservant un 'état' ou une mémoire des entrées antérieures dans la séquence. Cette caractéristique les rend idéaux pour des applications telles que le langage des signes où l'ordre et le contexte des gestes sont cruciaux.</li>
                            <li><strong>Adaptation au Projet :</strong> Dans notre projet, les RNN permettent d'interpréter une série de mouvements des mains en une séquence cohérente, capturant ainsi la dynamique temporelle du langage des signes.</li>
                        </ul>

                        <h4>3.1.2. LSTM :</h4>
                        <ul>
                            <li><strong>Architecture :</strong> Les LSTM sont une évolution des RNN traditionnels. Ils sont équipés de 'portes' qui régulent le flux d'informations, leur permettant de conserver ou d'oublier des informations de manière sélective.</li>
                            <li><strong>Adaptation au Projet :</strong> Leur capacité à gérer des dépendances à long terme est essentielle pour notre projet. Elle permet une interprétation précise des séquences de gestes, même complexes, en conservant le contexte sur toute la longueur de la séquence.</li>
                        </ul>

                        <h3>3.2 Résultat de RNN et LSTM</h3>

                        <h4>3.2.1. RNN :</h4>
                        <ul>
                            <li><strong>Performance :</strong> Les RNN ont montré une capacité remarquable à interpréter correctement des séquences de gestes courtes et simples. Cependant, ils rencontrent des difficultés avec des séquences plus longues et complexes en raison du problème de disparition du gradient.</li>
                            <li><strong>Évaluation :</strong> Ils ont enregistré une précision impressionnante de 99.78% sur l'ensemble de test, avec des scores parfaits de précision et de rappel.</li>
                        </ul>

                        <h4>3.2.2. LSTM :</h4>
                        <ul>
                            <li><strong>Performance :</strong> Les LSTM ont surpassé les RNN traditionnels dans la gestion des séquences longues et complexes grâce à leur capacité à maintenir des informations pertinentes sur de plus longues périodes.</li>
                            <li><strong>Évaluation :</strong> Ils ont atteint une précision de 96.72% avec des scores élevés de précision, de rappel et de score F1, démontrant leur compétence à classifier les gestes du langage des signes avec précision.</li>
                        </ul>

                        <h3>3.3 Limites du modèle RNN</h3>
                        <p>Suite à la précision assez "parfaite" du modèle RNN, souvent problématique, nous avons décidé de tester le modèle sous différents scénarios pour tester ses limites. Parmis ces scénarios :</p>
                        <ul>
                            <li><strong>Scénario 1 :</strong> Nous avons introduit des malfonctionnements de capteurs (en remplaçant leurs valeurs par des zeros par exemple) dans les données de test. Nous avons alors testé le modèle sur différentes combinaisons de capteurs malfonctionnants. Les resultats nous ont permis de déterminer quels capteurs sont les plus importants pour capturer les nuances des gestes du langage des signes, testant ainsi la robustesse du modèle face à des données incomplètes.</li>
                            <li><strong>Scénario 2 :</strong> Nous avons avons entrainé le modèle sur un ensemble de données plus petit, puis nous l'avons testé sur l'ensemble de test constitué du reste des données (enregistrées par une personne différente). Nous avons ainsi pu évaluer la capacité du modèle à généraliser et à s'adapter à des données plus variées, confirmant ainsi la non-présence d'un biais de sur-apprentissage.</li>
                            <li><strong>Scénario 3 :</strong> Nous avons entrainé le modèle sur un ensemble de données avec un nombre de capteurs réduit, se restraignant aux capteurs qui sont supposés garder des valeurs similaires pour les différents signes. L'objectif étant d'entrainer le modèle sur des données similaires, pour tester sa capacité à classifier les signes similaires, sans devoir ré-enregistrer de nouvelles données.</li>
                        </ul>
                        <p>Les résultats de ces scénarios sont présentés dans la section <a href="javascript:showContent('Notebooks')">Notebooks</a>.</p>
                        <p>
                            &nbsp; &nbsp; Une étape cruciale du projet a été l'intégration réussie du matériel (des gants équipés de capteurs) avec notre système d'intelligence artificielle. Cette intégration a permis de capturer avec précision les mouvements de la main. Pour plus de détails sur cette intégration et sur le matériel utilisé, veuillez consulter la section <a href="https://github.com/MouadFiali/magic-gloves/wiki">Wiki</a> de notre dépôt Github.
                        </p>
                </section>
    
                <section id="EvaluationEnv">
                    <h1>4. Évaluation fine des aspects environnementaux et sociétaux</h1>
                    <h4>4.1. Impacts environnementaux :</h4>
                    <div>
                    <ul>
                        <li><b>Fabrication des Dispositifs :</b> Les gants eux-mêmes, sont fabriqués à partir de polymères et de textiles synthétiques, peuvent générer des déchets non biodégradables. De plus, l'utilisation de composants électroniques tels que les capteurs, les Arduino, les câbles, etc., peut contribuer à l'augmentation des déchets électroniques.</li></br>
                        <li><b>Consommation d'Énergie :</b> Les dispositifs électroniques consomment de l'énergie, ce qui a des implications sur la demande énergétique globale. Il est crucial donc d'adopter des solutions économes en énergie pour minimiser l'impact environnemental.</li></br>
                        <li><b>Fin de Vie et Recyclage :</b> La conception des gants doit tenir compte de leur durabilité et de leur recyclabilité. Cependant, ce critère n'a pas été pris en compte dans notre conceptions. L'adoption de designs modulaires et l'utilisation de matériaux recyclables peuvent faciliter le recyclage et réduire l'impact environnemental.</li>
                    </ul>    
                    </div>
                    <h4>4.2. Impacts sociétaux :</h4>
                    <div>
                    <ul>
                        <li><b>Inclusion Sociale :</b> En brisant les barrières de communication, les 'Magic Gloves' ont le potentiel d'accroître significativement l'inclusion sociale des personnes sourdes ou muettes. Cela peut ouvrir de nouvelles opportunités dans divers domaines de la vie et réduire le sentiment d'isolement.</li></br>
                        <li><b>Opportunités Éducatives et Professionnelles :</b> L'amélioration de la communication peut ouvrir des portes vers de meilleures opportunités éducatives et professionnelles, contribuant ainsi à une société plus équitable.</li></br>
                        <li><b>Sensibilisation et Éducation :</b> Le projet peut également sensibiliser le public à la diversité des besoins en communication et encourager une meilleure compréhension et acceptation des différences.</li>
                    <ul>
                    </div>
                </section>

                <section id="bib">
                    <h1>Bibliographie</h1>
                    <ul class="bibliography">
                        <li id="kaggle-dataset" class="citation">
                            <span class="authors">Kaggle,</span>
                            <span class="title">“Datasets”</span>
                            <em><a href="https://www.kaggle.com/datasets/birdy654/sign-language-recognition-leap-motion" target="_blank">Leap Motion ASL and BSL Sign Language Recognition</a></em>
                        </li>
                    </ul>
                </section>

                `;



            var contentNotebooks = `<div id="notebooks-link">Pour accéder aux notebooks sur github, <a href="https://github.com/MouadFiali/magic-gloves/tree/main/notebooks" target="_blank">cliquez ici</a></div>
            </br></br>
            <ul class="nav nav-pills mb-3" id="pills-tab" role="tablist">
                <li class="nav-item">
                    <a class="nav-link active" id="pills-modelTesting1-tab" data-toggle="pill" href="#pills-modelTesting1" role="tab" aria-controls="pills-modelTesting1" aria-selected="true">Model Testing 1</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" id="pills-modelTesting2-tab" data-toggle="pill" href="#pills-modelTesting2" role="tab" aria-controls="pills-modelTesting2" aria-selected="false">Model Testing 2</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" id="pills-modelTesting3-tab" data-toggle="pill" href="#pills-modelTesting3" role="tab" aria-controls="pills-modelTesting3" aria-selected="false">Model Testing 3</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" id="pills-modelTesting4-tab" data-toggle="pill" href="#pills-modelTesting4" role="tab" aria-controls="pills-modelTesting4" aria-selected="false">Model Testing 4</a>
                </li>
            </ul>
            <div class="tab-content" id="pills-tabContent">
                <div class="tab-pane fade show active" id="pills-modelTesting1" role="tabpanel" aria-labelledby="pills-modelTesting1-tab"><iframe src="https://kamdrain.github.io/modeltesting1/" width="100%" height="600px"></iframe></div>
                <div class="tab-pane fade" id="pills-modelTesting2" role="tabpanel" aria-labelledby="pills-modelTesting2-tab"><iframe src="https://kamdrain.github.io/modeltesting2/" width="100%" height="600px"></iframe></div>
                <div class="tab-pane fade" id="pills-modelTesting3" role="tabpanel" aria-labelledby="pills-modelTesting3-tab"><iframe src="https://kamdrain.github.io/modeltesting3/" width="100%" height="600px"></iframe></div>
                <div class="tab-pane fade" id="pills-modelTesting4" role="tabpanel" aria-labelledby="pills-modelTesting4-tab"><iframe src="https://kamdrain.github.io/modeltesting4/" width="100%" height="600px"></iframe></div>
            </div>`;







            var contentGithub = `<h2>Code source et Dataset</h2>
            <table>
                <tr>
                    <td class="img-td">
                        <img src="https://foundations.projectpythia.org/_images/GitHub-logo.png" height="70px"  alt="Github logo">
                    </td>
                    <td class="text-td">
                        Pour accéder au code source sur github, <a href="https://github.com/MouadFiali/magic-gloves/tree/main" target="_blank">cliquez ici</a>
                    </td>
                </tr>
                <br/>
                <tr>
                    <td class="img-td">
                        <img src="https://cdn4.iconfinder.com/data/icons/logos-and-brands/512/189_Kaggle_logo_logos-512.png" height="70px" width="70px" alt="Kaggle logo">
                    </td>
                    <td class="text-td">
                        Pour accéder au dataset sur kaggle, <a href="https://www.kaggle.com/datasets/mouadfiali/sensor-based-american-sign-language-recognition" target="_blank">cliquez ici</a>
                    </td>
                </tr>
            </table>
            `;
            var contentAccueil = `
            <section id="accueil">
                <div class="welcome">
                    <img src="https://raw.githubusercontent.com/KamDrain/modeltesting1/main/last%20image%20in%20PNG.png"
                        width="100px" alt="Magic Gloves logo">
                    <h1 style="font-size: 65px;">Magic Gloves</h1>
                </div>
                <br />
                <div class="intro">
                    <p>&nbsp;&nbsp; Bienvenue dans l'univers innovant de Magic Gloves, un projet pionnier à l'intersection de l'Intelligence
                        Artificielle et de l'Internet des Objets (IoT). Notre mission est de révolutionner la communication en
                        brisant les barrières linguistiques, en particulier pour les personnes utilisant la langue des signes.
                        Notre projet ambitieux vise à traduire la langue des signes en langage parlé, en l'occurrence l'anglais,
                        rendant ainsi la communication plus accessible et inclusive pour tous.</p>
                    <p>&nbsp;&nbsp; Cette page offre un aperçu complet de Magic Gloves, présentant une exploration approfondie de ses
                        fondements conceptuels, développements techniques, et implications sociétales. Nous vous invitons à
                        parcourir les différentes sections pour une compréhension globale de notre démarche, de la genèse du
                        projet à ses applications pratiques, en passant par les innovations technologiques et les contributions
                        de notre équipe dédiée. Magic Gloves est plus qu'un projet - c'est une avancée vers une société plus
                        inclusive, où la technologie sert de pont entre les mondes du silence et du son.</p>
                    <p>Ce travail a été réalisé dans le cadre d'un projet académique combinant l'intelligence artificielle (IA) et l'Internet des Objets (IoT), illustrant une collaboration interdisciplinaire entre ces deux domaines.</p>
                    <div id="creators">
                        <div>
                            <h3>Réalisé par :</h3>
                            <ul>
                                <li>FIALI Mouad, <a href="mailto:mouad.fiali@grenoble-inp.org">mouad.fiali@grenoble-inp.org</a></li>
                                <li>GHAZAOUI Badr, <a href="mailto:badr.ghazaoui@grenoble-inp.org">badr.ghazaoui@grenoble-inp.org</a></li>
                                <li>MAROUANE Kamal, <a href="mailto:kamal.marouane@grenoble-inp.org">kamal.marouane@grenoble-inp.org</a></li>
                                <li>RIMAOUI Nabila, <a href="mailto:nabila.rimaoui@grenoble-inp.org">nabila.rimaoui@grenoble-inp.org</a></li></li>
                                <li>ZARKTOUNI Ismail, <a href="mailto:ismail.zarktouni@grenoble-inp.org">ismail.zarktouni@grenoble-inp.org</a></li></li></li>
                            </ul>
                        </div>
                        <div style="border-left: 3px solid black; height: 180px; margin-left: 50px; margin-right: 50px;"></div>
                        <div>
                            <h3>Encadré par :</h3>
                            <ul>
                                <li>M. Sylvain BOUVERET, <a href="mailto:sylvain.bouveret@imag.fr">sylvain.bouveret@imag.fr</a></li>
                                <li>M. Thibault TRICARD, <a href="mailto:thibault.tricard@inria.fr">thibault.tricard@inria.fr</a></li>
                                <li>M. Mathias RAMPARISON, <a href="mailto:mathias.ramparison@grenoble-inp.fr">mathias.ramparison@grenoble-inp.fr</a></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </section>
            `;
            var contentDemo = `<h2>Vidéo de démonstration:</h2><center><iframe src="https://drive.google.com/file/d/1rZYFMr-MuG-sYjRMZ9x--nTnDmlXHhr3/preview" width="800" height="480" allow="autoplay"></center>`;

            var content = {
                "Rapport": contentRapport,
                "Notebooks": contentNotebooks,
                "Annexe": contentGithub,
                "Accueil": contentAccueil,
                "Demo": contentDemo
            };

            document.getElementById("content").innerHTML = content[contentId];
        }
    </script>
    <link rel="shortcut icon" href="https://i.ibb.co/xJm1fcg/last-image.png">
</head>

<body>
    <!-- <div class="title">
        <img src="https://raw.githubusercontent.com/KamDrain/modeltesting1/main/last%20image%20in%20PNG.png" alt="Magic Gloves logo">
        <h1>Magic Gloves</h1>
    </div> -->
    <div class="menu">
        <ul>
            <li><a href="javascript:showContent('Accueil')">Accueil</a></li>
            <li><a href="javascript:showContent('Rapport')">Rapport</a></li>
            <li><a href="javascript:showContent('Notebooks')">Notebooks</a></li>
            <li><a href="javascript:showContent('Annexe')">Annexe</a></li>
            <li><a href="javascript:showContent('Demo')">Démonstration</a></li>
        </ul>
    </div>

    <div id="content">
        <script>
            document.addEventListener("DOMContentLoaded", function(event) {
                showContent('Accueil');
            });
        </script>
    </div>

</body>

</html>